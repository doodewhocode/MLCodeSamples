{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Word2Vec_v6.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"id":"iILSKkuFecfB"},"source":["# Word Embeddings"]},{"cell_type":"code","metadata":{"id":"0p7JavHfFCJq"},"source":["#Importing dependencies\n","import numpy as np\n","import tensorflow as tf\n","import pandas as pd\n","import tensorflow.keras as keras\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Activation\n","from tensorflow.keras.layers import Dropout\n","from tensorflow.keras.layers import RNN, SimpleRNN, LSTM\n","from tensorflow.keras.optimizers import RMSprop\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","import random\n","import matplotlib.pyplot as plt\n","%matplotlib inline \n","from mpl_toolkits import mplot3d\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e2qssnoOecfF"},"source":["# LAB1: Simple Word Embeddings in TensorFlow"]},{"cell_type":"code","metadata":{"id":"-GBHtv7ZecfG"},"source":["corpus = ['king is a strong man', \n","          'queen is a wise woman', \n","          'boy is a young man',\n","          'girl is a young woman', \n","          'prince is a young',\n","          'prince will be strong',\n","          'princess is young',\n","          'man is strong', \n","          'woman is pretty',\n","          'prince is a boy', \n","          'prince will be king',\n","          'princess is a girl',\n","          'princess will be queen']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5TgNn7sNecfM"},"source":["## Clean the corpus by removing stopwords"]},{"cell_type":"markdown","metadata":{"id":"ccs9zJ4decfN"},"source":["Remove stop words\n","In order for efficiency of creating word vector, we will remove commonly used words"]},{"cell_type":"code","metadata":{"id":"6cvZmkIzecfO"},"source":["def drop_stop_words(corpus):\n","    stop_words = ['is', 'a', 'will', 'be','was','and']\n","    results = []\n","    for text in corpus:\n","        tmp = text.split(' ')\n","        for stop_word in stop_words:\n","            if stop_word in tmp:\n","                tmp.remove(stop_word)\n","        results.append(\" \".join(tmp))\n","    \n","    return results"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"logJbbN2ecfS"},"source":["corpus_v1 = drop_stop_words(corpus)\n","print(corpus_v1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CJ3OnjyjecfX"},"source":["all_words=  [word for text in corpus_v1 for word in text.split(' ')]\n","words=set(all_words)\n","print(words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NO6iCvPrecfb"},"source":["word2int = {}\n","\n","for i,word in enumerate(words):\n","    word2int[word] = i\n","\n","sentences = []\n","for sentence in corpus_v1:\n","    sentences.append(sentence.split())\n","    \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-_utcxqFecfg"},"source":["## Data Preparation - Word to Context"]},{"cell_type":"code","metadata":{"id":"63jUwdu1ecfh"},"source":["WINDOW_SIZE = 2 #5-10\n","\n","data = []\n","for sentence in sentences:\n","    for idx, word in enumerate(sentence):\n","        for neighbor in sentence[max(idx - WINDOW_SIZE, 0) : min(idx + WINDOW_SIZE, len(sentence)) + 1] : \n","            if neighbor != word:\n","                data.append([word, neighbor])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BnPj-EEMecfk"},"source":["import pandas as pd\n","#for text in corpus_v1:\n","#    print(text)\n","\n","df = pd.DataFrame(data, columns = ['input', 'label'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qWCF44lEecfo"},"source":["print(df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vx7LYT30ecft"},"source":["df.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h1JJ2WBnecfx"},"source":["word2int"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oYP0zb2B4kz3"},"source":["## One-Hot encoding"]},{"cell_type":"code","metadata":{"id":"eMaRLp2uecf0"},"source":["ONE_HOT_DIM = len(words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P9j5KADkecf4"},"source":["def to_one_hot_encoding(data_point_index):\n","    one_hot_encoding = np.zeros(ONE_HOT_DIM)\n","    one_hot_encoding[data_point_index] = 1\n","    return one_hot_encoding\n","X = [] # input word\n","Y = [] # target word\n","\n","for x, y in zip(df['input'], df['label']):\n","    X.append(to_one_hot_encoding(word2int[ x ]))\n","    Y.append(to_one_hot_encoding(word2int[ y ]))\n","\n","# convert them to numpy arrays\n","X_train = np.asarray(X)\n","Y_train = np.asarray(Y)    \n","print(X_train.shape)\n","print(Y_train.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iSZZndntBEg_"},"source":["X_train.shape[1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"56gjxGWg4px2"},"source":["## Model Building"]},{"cell_type":"code","metadata":{"id":"n9iwp8Tbecf_"},"source":["model1 = Sequential()\n","model1.add(Dense(3, input_dim=X_train.shape[1], activation='sigmoid'))\n","model1.add(Dense(Y_train.shape[1] ,activation='softmax'))\n","model1.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xbI4p4YHecgD"},"source":["model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","# Train model\n","history = model1.fit(X_train, Y_train, epochs=600, batch_size=4 , verbose=0 )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GHs5gZSLecgH"},"source":["model1h = Sequential()\n","model1h.add(Dense(3, input_dim=X_train.shape[1], weights=model1.layers[0].get_weights()))\n","model1h.add(Activation('sigmoid'))\n","vectors_data = pd.DataFrame(model1h.predict(X_train))\n","vectors_data[\"word\"]=df[\"input\"]\n","vectors_data=vectors_data.drop_duplicates() \n","vectors_data.columns\n","print(vectors_data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ojyKM-KOecgN"},"source":["## Final Word2Vec Data visualization"]},{"cell_type":"code","metadata":{"id":"tPdtC2kNecgP"},"source":["w2v_df=pd.DataFrame()\n","w2v_df[\"word\"]=vectors_data[\"word\"]\n","w2v_df[\"x1\"]=vectors_data[0]\n","w2v_df[\"x2\"]=vectors_data[1]\n","w2v_df[\"x3\"]=vectors_data[2]\n","print(w2v_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zIGj_G3jecgT"},"source":["plt.rcParams.update({'font.size': 20})\n","\n","fig = plt.figure(figsize=(15,10))\n","ax = plt.axes(projection='3d')\n","ax = plt.axes(projection='3d')\n","\n","xdata = w2v_df[\"x1\"]\n","ydata = w2v_df[\"x2\"]\n","zdata = w2v_df[\"x3\"]\n","names=w2v_df[\"word\"]\n","\n","\n","ax.scatter3D(xdata, ydata, zdata, s=200 , c='green')\n","for names, x, y, z in zip(names, xdata, ydata, zdata):\n","    label = names\n","    ax.text(x, y, z, label )\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hskbgrqTecgY"},"source":["# LAB2 : Word Embeddings example in Gensim"]},{"cell_type":"code","metadata":{"id":"mEvhNshaecgb"},"source":["!pip install gensim\n","!pip install google.cloud"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ppu44oPNecgh"},"source":["#import gzip\n","import gensim"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JOCcY95jQmD4"},"source":["import urllib.request \n","#read cat image\n","urllib.request.urlretrieve(\"https://raw.githubusercontent.com/venkatareddykonasani/Datasets/master/word2vec_data/King_queen_v1.txt\", \"King_queen_v1.txt\")\n","data_file=\"King_queen_v1.txt\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vXLJ2-cZecgr"},"source":["def read_input(input_file):\n","   \n","    with open (input_file, 'rb') as f:\n","        for i, line in enumerate (f): \n","\n","            if (i%2==0):\n","                print(\"read {0} lines\".format (i))\n","            # do some pre-processing and return a list of words for each review text\n","            yield gensim.utils.simple_preprocess (line)\n","# read the tokenized reviews into a list\n","# each review item becomes a series of words\n","# so this becomes a list of lists\n","documents = list (read_input (data_file))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eD9vRQjiecgx"},"source":["print(documents)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pbjx851Kecg2"},"source":["from gensim.models import Word2Vec\n","model = Word2Vec(documents, min_count=1, size=3, window = 2)\n","#size： size of word vector, hidden layer\n","#min-count：discard words that appear less than # times\n","#window：Context Window size"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JoakJPqjecg8"},"source":["## Hyperparameters\n","\n","### size\n","The hidden nodes size. The size of the dense vector to represent each token or word. If you have very limited data, then size should be a much smaller value. If you have lots of data, its good to experiment with various sizes. \n","\n","### window\n","Context window size. The maximum distance between the target word and its neighboring word. If your neighbor's position is greater than the maximum window width to the left and the right, then, some neighbors are not considered as being related to the target word. In theory, a smaller window should give you terms that are more related. If you have lots of data, then the window size should not matter too much, as long as its a decent sized window.\n","\n","### min_count\n","Minimium frequency count of words. The model would ignore words that do not statisfy the min_count. Extremely infrequent words are usually unimportant, so its best to get rid of those. Unless your dataset is really tiny, this does not really affect the model."]},{"cell_type":"code","metadata":{"id":"Uhcoh2J4ecg-"},"source":["vectors=model[words]\n","print(vectors)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oJw9wiOWechC"},"source":["# access vector for one word\n","print(model['king'])\n","print(model['man'])\n","print(model['queen'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3-7-_qPxechI"},"source":["result=[print(word, model[word]) for word in words]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xr7ejMU8echP"},"source":["# save model\n","model.save(r\"model1.bin\")\n","# load model\n","new_model = Word2Vec.load(r\"model1.bin\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4wepiWwJ-o5j"},"source":["#Application of Word2Vec - Sentiment Analysis"]},{"cell_type":"markdown","metadata":{"id":"4dV2v4r6-85-"},"source":["[Word2_vec_Sentiment_Analysis code file](https://colab.research.google.com/drive/1666qEPApWzdw-efCu7JHhulF3MLRG2cw)"]},{"cell_type":"markdown","metadata":{"id":"TD4YeRxsechg"},"source":["# Pre trained model by google(3mn words)"]},{"cell_type":"markdown","metadata":{"id":"pC4Hou59tBcy"},"source":["## Load the model"]},{"cell_type":"code","metadata":{"id":"aUYSEbEEWiJ3"},"source":["from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0HlPb92OTZQH"},"source":["auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","downloaded = drive.CreateFile({'id':\"13e-z7BhTcm69qCjoFNehc6S0sc2qWod8\"})   \n","downloaded.GetContentFile(\"GoogleNews-vectors-negative300.zip\") \n","!unzip GoogleNews-vectors-negative300.zip\n","\n","#binfile- https://drive.google.com/file/d/1OfDK_9nUPYC1uCvscsr5z4cfBnWMrqX6/view?usp=sharing\n","#zipfile- https://drive.google.com/file/d/13e-z7BhTcm69qCjoFNehc6S0sc2qWod8/view?usp=sharing\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EA5K19txechh"},"source":["from gensim.models import KeyedVectors\n","# load the google word2vec model\n","filename=\"GoogleNews-vectors-negative300.bin\"\n","model = KeyedVectors.load_word2vec_format(filename, binary=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WIGn6NfGecho"},"source":["result = model.most_similar(positive=['congress', 'sonia'], topn=10)\n","print(result)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FtcEVdHNechs"},"source":["# look up top 6 words similar to 'polite'\n","w1 = [\"polite\"]\n","model.wv.most_similar (positive=w1,topn=6)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9yWE88bVechz"},"source":["# look up top 6 words similar to 'france'\n","w1 = [\"france\"]\n","model.wv.most_similar (positive=w1,topn=6)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cmTkdr2aech1"},"source":["w1 = [\"india\"]\n","model.wv.most_similar (positive=w1,topn=6)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sxAQzibbech4"},"source":["# look up top 6 words similar to 'shocked'\n","w1 = [\"dhoni\"]\n","model.wv.most_similar (positive=w1,topn=6)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ftYeeLVbeciA"},"source":["# similarity between two different words\n","model.wv.similarity(w1=\"dirty\",w2=\"smelly\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"grfHKd3veciC"},"source":["# similarity between two different words\n","model.wv.similarity(w1=\"dirty\",w2=\"great\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Fq32drDeciF"},"source":["# similarity between two different words\n","model.wv.similarity(w1=\"dirty\",w2=\"ugly\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WTHnOp7leciI"},"source":["# Which one is the odd one out in this list?\n","model.wv.doesnt_match([\"run\",\"walk\",\"france\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3z9LoANGeciM"},"source":["model.wv.doesnt_match([\"run\",\"india\",\"france\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CvGuQYv0echw"},"source":["w1 = [\"Nityananda\"]\n","model.wv.most_similar (positive=w1,topn=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-ElOfC9v_aYF"},"source":["w1 = [\"chess\"]\n","model.wv.most_similar (positive=w1,topn=6)"],"execution_count":null,"outputs":[]}]}